{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17414,"status":"ok","timestamp":1662213325270,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"},"user_tz":-480},"id":"NOtnP77-heVp","outputId":"9e018350-fa0f-4dba-ec66-dff57e23eb7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1662213325271,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"},"user_tz":-480},"id":"HqYi-aW6hkJ2","outputId":"e41dbf64-e829-4a9b-f2ed-768b40abbf7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/UCCD3074_Labs/Assignment2\n"]}],"source":["cd /content/drive/MyDrive/UCCD3074_Labs/Assignment2/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113810,"status":"ok","timestamp":1662213439074,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"},"user_tz":-480},"id":"dqYEQJMXhkYu","outputId":"fdb5ec04-1cc3-4d88-e93d-c2c5f03db807"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  6034  100  6034    0     0   9763      0 --:--:-- --:--:-- --:--:--  9763\n","Updating... This may take around 2 minutes.\n","Updating TPU runtime to pytorch-1.7 ...\n","Found existing installation: torch 1.12.1+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting cloud-tpu-client\n","  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client) (4.1.3)\n","Collecting google-api-python-client==1.8.0\n","  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.35.0)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.15.0)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.31.6)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (21.3)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2022.2.1)\n","Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.17.3)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (57.4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.56.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.9)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.9)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n","Uninstalling torch-1.12.1+cu113:\n","Installing collected packages: google-api-python-client, cloud-tpu-client\n","  Attempting uninstall: google-api-python-client\n","    Found existing installation: google-api-python-client 1.12.11\n","    Uninstalling google-api-python-client-1.12.11:\n","      Successfully uninstalled google-api-python-client-1.12.11\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","earthengine-api 0.1.321 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n","Done updating TPU runtime\n","  Successfully uninstalled torch-1.12.1+cu113\n","Found existing installation: torchvision 0.13.1+cu113\n","Uninstalling torchvision-0.13.1+cu113:\n","  Successfully uninstalled torchvision-0.13.1+cu113\n","Copying gs://tpu-pytorch/wheels/torch-1.7-cp37-cp37m-linux_x86_64.whl...\n","|\n","Operation completed over 1 objects/114.2 MiB.                                    \n","Copying gs://tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl...\n","\\ [1 files][127.4 MiB/127.4 MiB]                                                \n","Operation completed over 1 objects/127.4 MiB.                                    \n","Copying gs://tpu-pytorch/wheels/torchvision-1.7-cp37-cp37m-linux_x86_64.whl...\n","/ [1 files][  3.1 MiB/  3.1 MiB]                                                \n","Operation completed over 1 objects/3.1 MiB.                                      \n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./torch-1.7-cp37-cp37m-linux_x86_64.whl\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (1.21.6)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.16.0)\n","Installing collected packages: dataclasses, torch\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.9 requires torchvision>=0.8.2, which is not installed.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.7.0a0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.0a0 which is incompatible.\n","fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.7.0a0 which is incompatible.\u001b[0m\n","Successfully installed dataclasses-0.6 torch-1.7.0a0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n","Installing collected packages: torch-xla\n","Successfully installed torch-xla-1.7\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./torchvision-1.7-cp37-cp37m-linux_x86_64.whl\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchvision==1.7) (1.7.0a0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==1.7) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==1.7) (1.21.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==1.7) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==1.7) (4.1.1)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==1.7) (0.6)\n","Installing collected packages: torchvision\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 1.7.0a0 which is incompatible.\u001b[0m\n","Successfully installed torchvision-0.9.0a0+75e4a7d\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  libomp5\n","0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n","Need to get 234 kB of archives.\n","After this operation, 774 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n","Fetched 234 kB in 0s (725 kB/s)\n","Selecting previously unselected package libomp5:amd64.\n","(Reading database ... 155685 files and directories currently installed.)\n","Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n","Unpacking libomp5:amd64 (5.0.1-1) ...\n","Setting up libomp5:amd64 (5.0.1-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n","\u001b[K     |████████████████████████████████| 509 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.7.0a0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.0a0+75e4a7d)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (1.21.6)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (0.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: timm\n","Successfully installed timm-0.6.7\n"]}],"source":["############################\n","#Reference: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","#install TPU dependencies\n","!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py #transfer data to the notebook\n","!python pytorch-xla-env-setup.py --version 1.7 #get and setup torch_xla version\n","!pip install timm "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6277,"status":"ok","timestamp":1662213445342,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"},"user_tz":-480},"id":"lh2FjxwR6-SQ","outputId":"1e0eaa6f-2844-44b2-ffd6-0990baa53a67"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:TPU has started up successfully with version pytorch-1.7\n"]},{"output_type":"stream","name":"stdout","text":["torchversion: 1.7.0a0+7e71a98\n"]}],"source":["#import library\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","plt.style.use(\"ggplot\")\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","\n","import albumentations\n","\n","import torch_xla #to connect notebook to use Cloud TPU device\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.distributed.parallel_loader as pl\n","\n","import timm #to collect newest computer vision model\n","\n","import gc #garbage collector\n","import os #operating system\n","import time\n","import random #generate random number\n","from datetime import datetime\n","\n","from PIL import Image\n","from tqdm.notebook import tqdm #to create progress bar\n","from sklearn import model_selection, metrics\n","\n","# For parallelization in TPUs\n","os.environ[\"XLA_USE_BF16\"] = \"1\"\n","os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"\n","\n","print(\"torchversion:\",torch.__version__)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QUOVEMzqSEzq","executionInfo":{"status":"ok","timestamp":1662213445342,"user_tz":-480,"elapsed":7,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Coded by Ng Jiun Shen\n","############################\n","\n","# model specific global variables\n","Set = {\n","    'seed': 3074,\n","    'model_arch': 'vit_base_patch16_224',\n","    'img_size': 224,\n","    'epochs': 10,\n","    'train_bs': 16,\n","    'valid_bs': 16,\n","    'lr': 2e-05,\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":728,"status":"ok","timestamp":1662213446064,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"},"user_tz":-480},"id":"TkRGO8rKxEiJ","outputId":"e6ad8c22-67c9-4ba4-a775-22f9c7df8226"},"outputs":[{"output_type":"stream","name":"stdout","text":["top 5 records\n","          image_id  label\n","0  1000015157.jpg      0\n","1  1000201771.jpg      3\n","2   100042118.jpg      1\n","3  1000723321.jpg      1\n","4  1000812911.jpg      3\n","\n","last 5 records\n","             image_id  label\n","21392  999068805.jpg      3\n","21393  999329392.jpg      3\n","21394  999474432.jpg      1\n","21395  999616605.jpg      4\n","21396  999998473.jpg      4\n"]}],"source":["############################\n","#Coded by Ng Jiun Shen\n","############################\n","\n","#read file\n","df = pd.read_csv(\"../Assignment2/train.csv\")\n","\n","#check success loaded\n","print(\"top 5 records\\n\",df.head()) \n","print(\"\\nlast 5 records\\n\",df.tail()) "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1662213446065,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"},"user_tz":-480},"id":"wwETGANchki-","outputId":"c24bb268-fc5f-4e59-e218-e5f4da0871de"},"outputs":[{"output_type":"stream","name":"stdout","text":["dataset's length is 21397\n","trainset's length is 14978\n","validset's length is 3209\n","testset's length is 3210\n"]}],"source":["############################\n","#Coded by Leong Wai Yin\n","############################\n","\n","#Split into train,valid,test set\n","df_train = df.sample(frac=0.7, random_state=Set['seed'])\n","val_test = df.loc[~df.index.isin(df_train.index)]\n","df_test = val_test.sample(frac=0.5, random_state=Set['seed'])\n","df_valid = val_test.loc[~val_test.index.isin(df_test.index)]\n","print(\"dataset's length is\",len(df))\n","print(\"trainset's length is\",len(df_train))\n","print(\"validset's length is\",len(df_valid))\n","print(\"testset's length is\",len(df_test))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZAYTgcrghksu","executionInfo":{"status":"ok","timestamp":1662213446065,"user_tz":-480,"elapsed":6,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Reference: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","class CassavaDataset(torch.utils.data.Dataset): #class for dataset\n","    def __init__(self, df, data_path=\"../Assignment2\", mode=\"train\", transforms=None):\n","        super().__init__()\n","        self.df_data = df.values\n","        self.data_path = data_path\n","        self.transforms = transforms\n","        self.mode = mode\n","        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n","\n","    def __len__(self):\n","        return len(self.df_data)\n","\n","    def __getitem__(self, index):\n","        img_name, label = self.df_data[index] #assign index to each image\n","        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transforms is not None:\n","            image = self.transforms(img)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vsIQKHsR0lnj","executionInfo":{"status":"ok","timestamp":1662213446065,"user_tz":-480,"elapsed":5,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Adapted from <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","# create image augmentations\n","transforms_train = transforms.Compose(\n","    [\n","        transforms.Resize((Set['img_size'], Set['img_size'])),\n","        transforms.RandomRotation(0.1),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomVerticalFlip(p=0.5),\n","        transforms.RandomResizedCrop(Set['img_size']),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","    ]\n",")\n","\n","transforms_valid = transforms.Compose( #no augmentation in valid set\n","    [\n","        transforms.Resize((Set['img_size'], Set['img_size'])),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","    ]\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"zLHmXy8nhk1A","executionInfo":{"status":"ok","timestamp":1662213446612,"user_tz":-480,"elapsed":552,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Reference: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","class ViTBase16(nn.Module): #class for VIT module\n","    def __init__(self, n_classes, pretrained=False):\n","\n","        super(ViTBase16, self).__init__()\n","\n","        self.model = timm.create_model(Set['model_arch'], pretrained=False)\n","        if pretrained:\n","            self.model.load_state_dict(torch.load(\"../Assignment2/vit-base-models-pretrained-pytorch/jx_vit_base_p16_224-80ecf9dd.pth\"))\n","\n","        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","\n","    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n","        # keep track of training loss\n","        epoch_loss = 0.0\n","        epoch_accuracy = 0.0\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        self.model.train()\n","        for i, (data, target) in enumerate(train_loader):\n","            # move tensors to GPU if CUDA is available\n","            if device.type == \"cuda\":\n","                data, target = data.cuda(), target.cuda()\n","            elif device.type == \"xla\":\n","                data = data.to(device, dtype=torch.float32)\n","                target = target.to(device, dtype=torch.int64)\n","\n","            # clear the gradients of all optimized variables\n","            optimizer.zero_grad()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = self.forward(data)\n","            # calculate the batch loss\n","            loss = criterion(output, target)\n","            # backward pass: compute gradient of the loss with respect to model parameters\n","            loss.backward()\n","            # Calculate Accuracy\n","            accuracy = (output.argmax(dim=1) == target).float().mean()\n","            # update training loss and accuracy\n","            epoch_loss += loss\n","            epoch_accuracy += accuracy\n","\n","            # perform a single optimization step (parameter update)\n","            if device.type == \"xla\":\n","                xm.optimizer_step(optimizer)\n","\n","                if i % 20 == 0:\n","                    xm.master_print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n","\n","            else:\n","                optimizer.step()\n","\n","        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n","\n","    def validate_one_epoch(self, valid_loader, criterion, device):\n","        # keep track of validation loss\n","        valid_loss = 0.0\n","        valid_accuracy = 0.0\n","\n","        ######################\n","        # validate the model #\n","        ######################\n","        self.model.eval()\n","        for data, target in valid_loader:\n","            # move tensors to GPU if CUDA is available\n","            if device.type == \"cuda\":\n","                data, target = data.cuda(), target.cuda()\n","            elif device.type == \"xla\":\n","                data = data.to(device, dtype=torch.float32)\n","                target = target.to(device, dtype=torch.int64)\n","\n","            with torch.no_grad():\n","                # forward pass: compute predicted outputs by passing inputs to the model\n","                output = self.model(data)\n","                # calculate the batch loss\n","                loss = criterion(output, target)\n","                # Calculate Accuracy\n","                accuracy = (output.argmax(dim=1) == target).float().mean()\n","                # update average validation loss and accuracy\n","                valid_loss += loss\n","                valid_accuracy += accuracy\n","\n","        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"xfrqDIMbhk9i","executionInfo":{"status":"ok","timestamp":1662213446612,"user_tz":-480,"elapsed":6,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Reference: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","def fit_tpu(\n","    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n","):\n","\n","    valid_loss_min = np.Inf  # track change in validation loss\n","\n","    # keeping track of losses as it happen\n","    train_losses = []\n","    valid_losses = []\n","    train_accs = []\n","    valid_accs = []\n","\n","    for epoch in range(1, epochs + 1):\n","        gc.collect() #collect garbage data\n","        para_train_loader = pl.ParallelLoader(train_loader, [device]) #multiple TPU\n","\n","        xm.master_print(f\"{'='*50}\")\n","        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n","        train_loss, train_acc = model.train_one_epoch(\n","            para_train_loader.per_device_loader(device), criterion, optimizer, device\n","        )\n","        xm.master_print(\n","            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n \"\n","        )\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","        gc.collect()\n","\n","        if valid_loader is not None:\n","            gc.collect()\n","            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n","            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n","            valid_loss, valid_acc = model.validate_one_epoch(\n","                para_valid_loader.per_device_loader(device), criterion, device\n","            )\n","            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n","            valid_losses.append(valid_loss)\n","            valid_accs.append(valid_acc)\n","            gc.collect()\n","\n","            content = time.ctime() + ' ' + f'EPOCH {epoch}, [VALID] LOSS: {valid_loss},ACCURACY: {valid_acc}'\n","\n","            with open(f'log.txt', 'a') as appender:\n","                appender.write(content + '\\n')\n","\n","            # save model if validation loss has decreased\n","            if valid_loss <= valid_loss_min and epoch != 1:\n","                xm.master_print(\n","                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n","                        valid_loss_min, valid_loss\n","                    )\n","                )\n","            #                 xm.save(model.state_dict(), 'best_model.pth')\n","\n","            valid_loss_min = valid_loss\n","\n","    return {\n","        \"train_loss\": train_losses,\n","        \"valid_losses\": valid_losses,\n","        \"train_acc\": train_accs,\n","        \"valid_acc\": valid_accs,\n","    }"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"kANh2IGlhlFo","executionInfo":{"status":"ok","timestamp":1662213454711,"user_tz":-480,"elapsed":8105,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Reference: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","model = ViTBase16(n_classes=5, pretrained=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"qDk7lujThlU3","executionInfo":{"status":"ok","timestamp":1662213454712,"user_tz":-480,"elapsed":10,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[],"source":["############################\n","#Adapted from: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","def _run():\n","    train_dataset = CassavaDataset(df_train, transforms=transforms_train)\n","    valid_dataset = CassavaDataset(df_valid, transforms=transforms_valid)\n","\n","    train_sampler = torch.utils.data.distributed.DistributedSampler(\n","        train_dataset,\n","        num_replicas=xm.xrt_world_size(),\n","        rank=xm.get_ordinal(),\n","        shuffle=True,\n","    )\n","\n","    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n","        valid_dataset,\n","        num_replicas=xm.xrt_world_size(),\n","        rank=xm.get_ordinal(),\n","        shuffle=False,\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=Set['train_bs'],\n","        sampler=train_sampler,\n","        drop_last=True,\n","        num_workers=8,\n","    )\n","\n","    valid_loader = torch.utils.data.DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=Set['valid_bs'],\n","        sampler=valid_sampler,\n","        drop_last=True,\n","        num_workers=8,\n","    )\n","\n","    criterion = nn.CrossEntropyLoss()\n","    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    device = xm.xla_device()\n","    model.to(device)\n","\n","    lr = Set['lr'] * xm.xrt_world_size()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n","    start_time = datetime.now()\n","    xm.master_print(f\"Start Time: {start_time}\")\n","\n","    logs = fit_tpu(\n","        model=model,\n","        epochs=Set['epochs'],\n","        device=device,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        train_loader=train_loader,\n","        valid_loader=valid_loader,\n","    )\n","\n","    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n","\n","    xm.master_print(\"Saving Model\")\n","    xm.save(\n","        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n","    )"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91-lUdUhhlcb","outputId":"22a7cfc6-c393-4cc1-b7c2-89c7a333bc51","executionInfo":{"status":"ok","timestamp":1662217081554,"user_tz":-480,"elapsed":3626850,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["INITIALIZING TRAINING ON 8 TPU CORES\n","Start Time: 2022-09-03 13:57:49.172921\n","==================================================\n","EPOCH 1 - TRAINING...\n","\tBATCH 1/117 - LOSS: 1.3671875\n","\tBATCH 21/117 - LOSS: 0.85546875\n","\tBATCH 41/117 - LOSS: 0.546875\n","\tBATCH 61/117 - LOSS: 0.60546875\n","\tBATCH 81/117 - LOSS: 0.67578125\n","\tBATCH 101/117 - LOSS: 0.75390625\n","\n","\t[TRAIN] EPOCH 1 - LOSS: 0.65234375, ACCURACY: 0.78515625\n"," \n","EPOCH 1 - VALIDATING...\n","\t[VALID] LOSS: 0.55078125, ACCURACY: 0.7890625\n","\n","==================================================\n","EPOCH 2 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.1767578125\n","\tBATCH 21/117 - LOSS: 0.390625\n","\tBATCH 41/117 - LOSS: 0.3125\n","\tBATCH 61/117 - LOSS: 0.466796875\n","\tBATCH 81/117 - LOSS: 0.3203125\n","\tBATCH 101/117 - LOSS: 0.7890625\n","\n","\t[TRAIN] EPOCH 2 - LOSS: 0.453125, ACCURACY: 0.87109375\n"," \n","EPOCH 2 - VALIDATING...\n","\t[VALID] LOSS: 0.50390625, ACCURACY: 0.82421875\n","\n","Validation loss decreased (0.5508 --> 0.5039).  Saving model ...\n","==================================================\n","EPOCH 3 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.2158203125\n","\tBATCH 21/117 - LOSS: 0.50390625\n","\tBATCH 41/117 - LOSS: 0.228515625\n","\tBATCH 61/117 - LOSS: 0.287109375\n","\tBATCH 81/117 - LOSS: 0.6171875\n","\tBATCH 101/117 - LOSS: 0.79296875\n","\n","\t[TRAIN] EPOCH 3 - LOSS: 0.416015625, ACCURACY: 0.8828125\n"," \n","EPOCH 3 - VALIDATING...\n","\t[VALID] LOSS: 0.5234375, ACCURACY: 0.7890625\n","\n","==================================================\n","EPOCH 4 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.162109375\n","\tBATCH 21/117 - LOSS: 0.279296875\n","\tBATCH 41/117 - LOSS: 0.2158203125\n","\tBATCH 61/117 - LOSS: 0.130859375\n","\tBATCH 81/117 - LOSS: 0.47265625\n","\tBATCH 101/117 - LOSS: 0.609375\n","\n","\t[TRAIN] EPOCH 4 - LOSS: 0.3671875, ACCURACY: 0.921875\n"," \n","EPOCH 4 - VALIDATING...\n","\t[VALID] LOSS: 0.5, ACCURACY: 0.828125\n","\n","Validation loss decreased (0.5234 --> 0.5000).  Saving model ...\n","==================================================\n","EPOCH 5 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.232421875\n","\tBATCH 21/117 - LOSS: 0.458984375\n","\tBATCH 41/117 - LOSS: 0.294921875\n","\tBATCH 61/117 - LOSS: 0.2314453125\n","\tBATCH 81/117 - LOSS: 0.3984375\n","\tBATCH 101/117 - LOSS: 0.64453125\n","\n","\t[TRAIN] EPOCH 5 - LOSS: 0.376953125, ACCURACY: 0.90234375\n"," \n","EPOCH 5 - VALIDATING...\n","\t[VALID] LOSS: 0.498046875, ACCURACY: 0.8203125\n","\n","Validation loss decreased (0.5000 --> 0.4980).  Saving model ...\n","==================================================\n","EPOCH 6 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.1142578125\n","\tBATCH 21/117 - LOSS: 0.265625\n","\tBATCH 41/117 - LOSS: 0.375\n","\tBATCH 61/117 - LOSS: 0.51171875\n","\tBATCH 81/117 - LOSS: 0.443359375\n","\tBATCH 101/117 - LOSS: 0.703125\n","\n","\t[TRAIN] EPOCH 6 - LOSS: 0.361328125, ACCURACY: 0.94140625\n"," \n","EPOCH 6 - VALIDATING...\n","\t[VALID] LOSS: 0.470703125, ACCURACY: 0.8203125\n","\n","Validation loss decreased (0.4980 --> 0.4707).  Saving model ...\n","==================================================\n","EPOCH 7 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.267578125\n","\tBATCH 21/117 - LOSS: 0.56640625\n","\tBATCH 41/117 - LOSS: 0.091796875\n","\tBATCH 61/117 - LOSS: 0.125\n","\tBATCH 81/117 - LOSS: 0.390625\n","\tBATCH 101/117 - LOSS: 0.65234375\n","\n","\t[TRAIN] EPOCH 7 - LOSS: 0.349609375, ACCURACY: 0.91796875\n"," \n","EPOCH 7 - VALIDATING...\n","\t[VALID] LOSS: 0.439453125, ACCURACY: 0.84375\n","\n","Validation loss decreased (0.4707 --> 0.4395).  Saving model ...\n","==================================================\n","EPOCH 8 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.1015625\n","\tBATCH 21/117 - LOSS: 0.1982421875\n","\tBATCH 41/117 - LOSS: 0.140625\n","\tBATCH 61/117 - LOSS: 0.45703125\n","\tBATCH 81/117 - LOSS: 0.390625\n","\tBATCH 101/117 - LOSS: 0.431640625\n","\n","\t[TRAIN] EPOCH 8 - LOSS: 0.302734375, ACCURACY: 0.95703125\n"," \n","EPOCH 8 - VALIDATING...\n","\t[VALID] LOSS: 0.484375, ACCURACY: 0.8359375\n","\n","==================================================\n","EPOCH 9 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.14453125\n","\tBATCH 21/117 - LOSS: 0.30859375\n","\tBATCH 41/117 - LOSS: 0.177734375\n","\tBATCH 61/117 - LOSS: 0.2236328125\n","\tBATCH 81/117 - LOSS: 0.3125\n","\tBATCH 101/117 - LOSS: 0.59375\n","\n","\t[TRAIN] EPOCH 9 - LOSS: 0.310546875, ACCURACY: 0.94140625\n"," \n","EPOCH 9 - VALIDATING...\n","\t[VALID] LOSS: 0.470703125, ACCURACY: 0.8515625\n","\n","Validation loss decreased (0.4844 --> 0.4707).  Saving model ...\n","==================================================\n","EPOCH 10 - TRAINING...\n","\tBATCH 1/117 - LOSS: 0.0791015625\n","\tBATCH 21/117 - LOSS: 0.32421875\n","\tBATCH 41/117 - LOSS: 0.197265625\n","\tBATCH 61/117 - LOSS: 0.294921875\n","\tBATCH 81/117 - LOSS: 0.34375\n","\tBATCH 101/117 - LOSS: 0.63671875\n","\n","\t[TRAIN] EPOCH 10 - LOSS: 0.29296875, ACCURACY: 0.9375\n"," \n","EPOCH 10 - VALIDATING...\n","\t[VALID] LOSS: 0.435546875, ACCURACY: 0.8515625\n","\n","Validation loss decreased (0.4707 --> 0.4355).  Saving model ...\n","Execution time: 1:00:01.101071\n","Saving Model\n"]}],"source":["############################\n","#Reference: <https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline/notebook>\n","############################\n","\n","# Start training processes\n","def _mp_fn(rank, flags):\n","    torch.set_default_tensor_type(\"torch.FloatTensor\")\n","    a = _run()\n","\n","\n","# _run()\n","FLAGS = {}\n","xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNxVzv5NbzB3","executionInfo":{"status":"ok","timestamp":1662217081555,"user_tz":-480,"elapsed":8,"user":{"displayName":"ng jiunshen","userId":"08738597098667889039"}},"outputId":"3a0e942f-e55a-4adb-9293-e0ad37017ad7"},"outputs":[{"output_type":"stream","name":"stdout","text":[".LOGSat Sep  3 13:14:36 2022 EPOCH 1, [VALID] LOSS: 0.455078125,ACCURACY: 0.8515625\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.52734375,ACCURACY: 0.80859375\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.44921875,ACCURACY: 0.85546875\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.5234375,ACCURACY: 0.828125\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.50390625,ACCURACY: 0.828125\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.431640625,ACCURACY: 0.8203125\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.55078125,ACCURACY: 0.8203125\n","Sat Sep  3 13:14:37 2022 EPOCH 1, [VALID] LOSS: 0.404296875,ACCURACY: 0.87109375\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.515625,ACCURACY: 0.83984375\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.390625,ACCURACY: 0.86328125\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.45703125,ACCURACY: 0.828125\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.4453125,ACCURACY: 0.83984375\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.375,ACCURACY: 0.890625\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.4921875,ACCURACY: 0.828125\n","Sat Sep  3 13:21:10 2022 EPOCH 2, [VALID] LOSS: 0.41796875,ACCURACY: 0.84375\n","Sat Sep  3 13:21:11 2022 EPOCH 2, [VALID] LOSS: 0.46484375,ACCURACY: 0.83984375\n","Sat Sep  3 13:26:55 2022 EPOCH 3, [VALID] LOSS: 0.3828125,ACCURACY: 0.86328125\n","Sat Sep  3 13:26:55 2022 EPOCH 3, [VALID] LOSS: 0.4765625,ACCURACY: 0.8359375\n","Sat Sep  3 13:26:55 2022 EPOCH 3, [VALID] LOSS: 0.515625,ACCURACY: 0.8203125\n","Sat Sep  3 13:26:55 2022 EPOCH 3, [VALID] LOSS: 0.494140625,ACCURACY: 0.828125\n","Sat Sep  3 13:26:56 2022 EPOCH 3, [VALID] LOSS: 0.431640625,ACCURACY: 0.8515625\n","Sat Sep  3 13:26:56 2022 EPOCH 3, [VALID] LOSS: 0.453125,ACCURACY: 0.8359375\n","Sat Sep  3 13:26:56 2022 EPOCH 3, [VALID] LOSS: 0.3984375,ACCURACY: 0.859375\n","Sat Sep  3 13:26:56 2022 EPOCH 3, [VALID] LOSS: 0.357421875,ACCURACY: 0.890625\n","Sat Sep  3 13:49:56 2022 EPOCH 1, [VALID] LOSS: 0.52734375,ACCURACY: 0.80078125\n","Sat Sep  3 13:49:57 2022 EPOCH 1, [VALID] LOSS: 0.53515625,ACCURACY: 0.8203125\n","Sat Sep  3 13:49:57 2022 EPOCH 1, [VALID] LOSS: 0.447265625,ACCURACY: 0.83984375\n","Sat Sep  3 13:49:57 2022 EPOCH 1, [VALID] LOSS: 0.50390625,ACCURACY: 0.828125\n","Sat Sep  3 13:49:57 2022 EPOCH 1, [VALID] LOSS: 0.40234375,ACCURACY: 0.83984375\n","Sat Sep  3 13:49:57 2022 EPOCH 1, [VALID] LOSS: 0.4453125,ACCURACY: 0.83984375\n","Sat Sep  3 13:49:57 2022 EPOCH 1, [VALID] LOSS: 0.5390625,ACCURACY: 0.8203125\n","Sat Sep  3 13:49:58 2022 EPOCH 1, [VALID] LOSS: 0.474609375,ACCURACY: 0.83984375\n","Sat Sep  3 14:04:53 2022 EPOCH 1, [VALID] LOSS: 0.515625,ACCURACY: 0.80859375\n","Sat Sep  3 14:04:54 2022 EPOCH 1, [VALID] LOSS: 0.51171875,ACCURACY: 0.82421875\n","Sat Sep  3 14:04:56 2022 EPOCH 1, [VALID] LOSS: 0.4765625,ACCURACY: 0.828125\n","Sat Sep  3 14:04:56 2022 EPOCH 1, [VALID] LOSS: 0.431640625,ACCURACY: 0.85546875\n","Sat Sep  3 14:04:57 2022 EPOCH 1, [VALID] LOSS: 0.55078125,ACCURACY: 0.7890625\n","Sat Sep  3 14:04:57 2022 EPOCH 1, [VALID] LOSS: 0.54296875,ACCURACY: 0.8203125\n","Sat Sep  3 14:04:57 2022 EPOCH 1, [VALID] LOSS: 0.4453125,ACCURACY: 0.83984375\n","Sat Sep  3 14:04:57 2022 EPOCH 1, [VALID] LOSS: 0.59375,ACCURACY: 0.80859375\n","Sat Sep  3 14:10:49 2022 EPOCH 2, [VALID] LOSS: 0.47265625,ACCURACY: 0.828125\n","Sat Sep  3 14:10:49 2022 EPOCH 2, [VALID] LOSS: 0.4140625,ACCURACY: 0.8515625\n","Sat Sep  3 14:10:49 2022 EPOCH 2, [VALID] LOSS: 0.38671875,ACCURACY: 0.87890625\n","Sat Sep  3 14:10:49 2022 EPOCH 2, [VALID] LOSS: 0.50390625,ACCURACY: 0.8359375\n","Sat Sep  3 14:10:50 2022 EPOCH 2, [VALID] LOSS: 0.46484375,ACCURACY: 0.8203125\n","Sat Sep  3 14:10:50 2022 EPOCH 2, [VALID] LOSS: 0.50390625,ACCURACY: 0.82421875\n","Sat Sep  3 14:10:50 2022 EPOCH 2, [VALID] LOSS: 0.431640625,ACCURACY: 0.828125\n","Sat Sep  3 14:10:50 2022 EPOCH 2, [VALID] LOSS: 0.5234375,ACCURACY: 0.828125\n","Sat Sep  3 14:16:59 2022 EPOCH 3, [VALID] LOSS: 0.494140625,ACCURACY: 0.82421875\n","Sat Sep  3 14:16:59 2022 EPOCH 3, [VALID] LOSS: 0.40234375,ACCURACY: 0.87109375\n","Sat Sep  3 14:17:00 2022 EPOCH 3, [VALID] LOSS: 0.494140625,ACCURACY: 0.8203125\n","Sat Sep  3 14:17:00 2022 EPOCH 3, [VALID] LOSS: 0.470703125,ACCURACY: 0.828125\n","Sat Sep  3 14:17:00 2022 EPOCH 3, [VALID] LOSS: 0.380859375,ACCURACY: 0.88671875\n","Sat Sep  3 14:17:00 2022 EPOCH 3, [VALID] LOSS: 0.55078125,ACCURACY: 0.82421875\n","Sat Sep  3 14:17:00 2022 EPOCH 3, [VALID] LOSS: 0.427734375,ACCURACY: 0.828125\n","Sat Sep  3 14:17:00 2022 EPOCH 3, [VALID] LOSS: 0.5234375,ACCURACY: 0.7890625\n","Sat Sep  3 14:22:47 2022 EPOCH 4, [VALID] LOSS: 0.384765625,ACCURACY: 0.875\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.419921875,ACCURACY: 0.83984375\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.470703125,ACCURACY: 0.84375\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.4609375,ACCURACY: 0.8203125\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.375,ACCURACY: 0.88671875\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.5078125,ACCURACY: 0.80859375\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.5390625,ACCURACY: 0.80859375\n","Sat Sep  3 14:22:48 2022 EPOCH 4, [VALID] LOSS: 0.5,ACCURACY: 0.828125\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.5390625,ACCURACY: 0.8203125\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.498046875,ACCURACY: 0.8203125\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.447265625,ACCURACY: 0.828125\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.359375,ACCURACY: 0.890625\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.404296875,ACCURACY: 0.83984375\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.4453125,ACCURACY: 0.8515625\n","Sat Sep  3 14:28:33 2022 EPOCH 5, [VALID] LOSS: 0.48828125,ACCURACY: 0.8359375\n","Sat Sep  3 14:28:34 2022 EPOCH 5, [VALID] LOSS: 0.36328125,ACCURACY: 0.875\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.4765625,ACCURACY: 0.83984375\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.35546875,ACCURACY: 0.88671875\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.5390625,ACCURACY: 0.828125\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.419921875,ACCURACY: 0.828125\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.455078125,ACCURACY: 0.828125\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.470703125,ACCURACY: 0.8203125\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.466796875,ACCURACY: 0.83984375\n","Sat Sep  3 14:34:14 2022 EPOCH 6, [VALID] LOSS: 0.359375,ACCURACY: 0.890625\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.34765625,ACCURACY: 0.88671875\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.51953125,ACCURACY: 0.84375\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.45703125,ACCURACY: 0.828125\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.4296875,ACCURACY: 0.8515625\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.345703125,ACCURACY: 0.89453125\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.427734375,ACCURACY: 0.8359375\n","Sat Sep  3 14:40:06 2022 EPOCH 7, [VALID] LOSS: 0.4296875,ACCURACY: 0.8515625\n","Sat Sep  3 14:40:07 2022 EPOCH 7, [VALID] LOSS: 0.439453125,ACCURACY: 0.84375\n","Sat Sep  3 14:45:52 2022 EPOCH 8, [VALID] LOSS: 0.490234375,ACCURACY: 0.8203125\n","Sat Sep  3 14:45:52 2022 EPOCH 8, [VALID] LOSS: 0.345703125,ACCURACY: 0.88671875\n","Sat Sep  3 14:45:52 2022 EPOCH 8, [VALID] LOSS: 0.54296875,ACCURACY: 0.828125\n","Sat Sep  3 14:45:53 2022 EPOCH 8, [VALID] LOSS: 0.427734375,ACCURACY: 0.84375\n","Sat Sep  3 14:45:53 2022 EPOCH 8, [VALID] LOSS: 0.4453125,ACCURACY: 0.828125\n","Sat Sep  3 14:45:53 2022 EPOCH 8, [VALID] LOSS: 0.34765625,ACCURACY: 0.89453125\n","Sat Sep  3 14:45:53 2022 EPOCH 8, [VALID] LOSS: 0.41015625,ACCURACY: 0.8515625\n","Sat Sep  3 14:45:53 2022 EPOCH 8, [VALID] LOSS: 0.484375,ACCURACY: 0.8359375\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.44921875,ACCURACY: 0.83984375\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.33984375,ACCURACY: 0.91015625\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.453125,ACCURACY: 0.8359375\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.41796875,ACCURACY: 0.84375\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.53125,ACCURACY: 0.83984375\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.490234375,ACCURACY: 0.8203125\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.357421875,ACCURACY: 0.88671875\n","Sat Sep  3 14:51:42 2022 EPOCH 9, [VALID] LOSS: 0.470703125,ACCURACY: 0.8515625\n","Sat Sep  3 14:57:49 2022 EPOCH 10, [VALID] LOSS: 0.4296875,ACCURACY: 0.828125\n","Sat Sep  3 14:57:49 2022 EPOCH 10, [VALID] LOSS: 0.46484375,ACCURACY: 0.8515625\n","Sat Sep  3 14:57:49 2022 EPOCH 10, [VALID] LOSS: 0.34765625,ACCURACY: 0.89453125\n","Sat Sep  3 14:57:49 2022 EPOCH 10, [VALID] LOSS: 0.447265625,ACCURACY: 0.83984375\n","Sat Sep  3 14:57:50 2022 EPOCH 10, [VALID] LOSS: 0.357421875,ACCURACY: 0.890625\n","Sat Sep  3 14:57:50 2022 EPOCH 10, [VALID] LOSS: 0.515625,ACCURACY: 0.8515625\n","Sat Sep  3 14:57:50 2022 EPOCH 10, [VALID] LOSS: 0.427734375,ACCURACY: 0.859375\n","Sat Sep  3 14:57:50 2022 EPOCH 10, [VALID] LOSS: 0.435546875,ACCURACY: 0.8515625\n","\n"]}],"source":["############################\n","#Coded by Ng Jiun Shen\n","############################\n","\n","f = open(f'./log.txt', \"r\")\n","\n","print(f.read())"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyODRdJU+vNhURvdf6CBaNZ9"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}